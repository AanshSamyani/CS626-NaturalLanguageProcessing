{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import nltk\n",
    "from nltk.corpus import inaugural\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('brown')\n",
    "nltk.download('punkt')\n",
    "nltk.download('universal_tagset')\n",
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'DET'), ('Fulton', 'NOUN'), ('County', 'NOUN'), ('Grand', 'ADJ'), ('Jury', 'NOUN'), ('said', 'VERB'), ('Friday', 'NOUN'), ('an', 'DET'), ('investigation', 'NOUN'), ('of', 'ADP'), (\"Atlanta's\", 'NOUN'), ('recent', 'ADJ'), ('primary', 'NOUN'), ('election', 'NOUN'), ('produced', 'VERB'), ('``', '.'), ('no', 'DET'), ('evidence', 'NOUN'), (\"''\", '.'), ('that', 'ADP'), ('any', 'DET'), ('irregularities', 'NOUN'), ('took', 'VERB'), ('place', 'NOUN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "## Data Processing\n",
    "tagged_sentences = brown.tagged_sents(tagset='universal')\n",
    "num_sentences = len(brown.tagged_sents())\n",
    "\n",
    "START_SYMBOL = '^'\n",
    "END_SYMBOL = '.'\n",
    "\n",
    "sentences = []\n",
    "for i in range(num_sentences):\n",
    "    sentences.append(tagged_sentences[i])\n",
    "    \n",
    "for i in range(num_sentences):\n",
    "    for j in range(len(sentences[i])):\n",
    "        x = list(sentences[i][j])\n",
    "        x[0] = x[0].lower()\n",
    "        sentences[i].pop(j)\n",
    "        sentences[i].insert(j, tuple(x))\n",
    "        \n",
    "for i in range(num_sentences):\n",
    "    sentences[i] = [(START_SYMBOL, START_SYMBOL)] + sentences[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Transition Probabilities\n",
    "count_tags_bigram = {}\n",
    "for j in range(len(sentences)):\n",
    "    for i, item in enumerate(sentences[j]):\n",
    "        if(i == len(sentences[j])-1):\n",
    "            continue\n",
    "        \n",
    "        tuple_tags = (sentences[j][i+1][1], sentences[j][i][1])\n",
    "        if tuple_tags in count_tags_bigram.keys():\n",
    "            count_tags_bigram[tuple_tags] += 1\n",
    "            \n",
    "        else:\n",
    "            count_tags_bigram[tuple_tags] = 1\n",
    "            \n",
    "probability_tags_bigram = {}\n",
    "total = 0\n",
    "for key in count_tags_bigram.keys():\n",
    "    total += count_tags_bigram[key]\n",
    "    \n",
    "    \n",
    "for key in count_tags_bigram.keys():\n",
    "    probability_tags_bigram[key] = count_tags_bigram[key]/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Lexical Independence Probabilities / Emission Probability\n",
    "emission_probabilities_dict = {'.' : {'.' : 1}}\n",
    "\n",
    "for j in range(len(sentences)):\n",
    "    for i, item in enumerate(sentences[j]):\n",
    "        if(i == len(sentences[j])-1):\n",
    "            continue\n",
    "        \n",
    "        if(sentences[j][i][1] in emission_probabilities_dict):\n",
    "            if(sentences[j][i][0] in emission_probabilities_dict[sentences[j][i][1]]):\n",
    "                emission_probabilities_dict[sentences[j][i][1]][sentences[j][i][0]] += 1\n",
    "                \n",
    "            else:\n",
    "                emission_probabilities_dict[sentences[j][i][1]][sentences[j][i][0]] = 1\n",
    "                \n",
    "        else:\n",
    "            emission_probabilities_dict[sentences[j][i][1]] = {}\n",
    "            emission_probabilities_dict[sentences[j][i][1]][sentences[j][i][0]] = 1\n",
    "            \n",
    "tags = list(emission_probabilities_dict.keys())\n",
    "for key1 in emission_probabilities_dict.keys():\n",
    "    total = 0\n",
    "    for key2 in emission_probabilities_dict[key1].keys():\n",
    "        total += emission_probabilities_dict[key1][key2]\n",
    "    \n",
    "    for key3 in emission_probabilities_dict[key1].keys():\n",
    "        emission_probabilities_dict[key1][key3] /= total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['reading', 'is', 'an', 'essential', 'skill', '.']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_sent = \"reading is an essential skill.\"\n",
    "input_sent_tokenized = word_tokenize(input_sent)\n",
    "\n",
    "input_sent_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "viterbi_matrix = []\n",
    "ls = []\n",
    "for tag in tags:\n",
    "    if((tag, '^') in probability_tags_bigram.keys()):\n",
    "        if(input_sent_tokenized[0] in emission_probabilities_dict[tag].keys()):\n",
    "            ls.append(('^', probability_tags_bigram[(tag, '^')] * emission_probabilities_dict[tag][input_sent_tokenized[0]]))\n",
    "        \n",
    "        else:\n",
    "            ls.append(('^', 0))\n",
    "    \n",
    "    else:\n",
    "        ls.append(('^', 0))\n",
    "        \n",
    "viterbi_matrix.append(ls)\n",
    "\n",
    "tags = list(tags)\n",
    "for i in range(1, len(input_sent_tokenized)):\n",
    "    ls = [] \n",
    "    for tag in tags:\n",
    "        max_probability = 0\n",
    "        max_tag = '$Can$'\n",
    "        for j in range(len(viterbi_matrix[i-1])):\n",
    "            if(viterbi_matrix[i-1][j][1] > 0):\n",
    "                prev_tag = tags[j]\n",
    "                prev_probability = viterbi_matrix[i-1][j][1]\n",
    "                \n",
    "                if (tag, prev_tag) in probability_tags_bigram.keys() and input_sent_tokenized[i] in emission_probabilities_dict[tag].keys():\n",
    "                    total_probability  = prev_probability * probability_tags_bigram[(tag, prev_tag)] * emission_probabilities_dict[tag][input_sent_tokenized[i]]\n",
    "                    if(total_probability > max_probability):\n",
    "                        max_probability = total_probability\n",
    "                        max_tag = prev_tag            \n",
    "        \n",
    "        ls.append((max_tag, max_probability))                \n",
    "    viterbi_matrix.append(ls)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start: ^\n",
      "reading: NOUN\n",
      "is: VERB\n",
      "an: DET\n",
      "essential: ADJ\n",
      "skill: NOUN\n"
     ]
    }
   ],
   "source": [
    "for i,item in enumerate(viterbi_matrix):\n",
    "    max_val = -1\n",
    "    max_tag = ''\n",
    "    for x in item:\n",
    "        if(x[1] > max_val):\n",
    "            max_val = x[1]\n",
    "            max_tag = x[0]\n",
    "    if(i == 0): print('Start: ' + max_tag)\n",
    "    else: print(input_sent_tokenized[i-1] + \": \" + max_tag)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
